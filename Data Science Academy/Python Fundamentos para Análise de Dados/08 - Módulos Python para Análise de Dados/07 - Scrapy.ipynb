{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### É necessário instalar os módulos Scrapy e PyMongo:\n",
    "```sh\n",
    "pip install scrapy\n",
    "#ou \n",
    "conda install -c anaconda scrapy\n",
    "```\n",
    "```sh\n",
    "pip install pymongo\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "O Jupyter neste caso serve só de bloco de notas. Todo o trabalho foi realizado em terminal com ajuda de um editor de texto (VSCode)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Etapas:\n",
    "1. Criar projeto\n",
    "```sh\n",
    "scrapy startproject \"nome do projeto\"\n",
    "# neste caso\n",
    "scrapy startproject stack\n",
    "```\n",
    "2. Entrar no projeto\n",
    "```sh\n",
    "cd \"nome do projeto\"\n",
    "# neste caso\n",
    "cd stack\n",
    "```\n",
    "\n",
    "3. Editar os arquivos\n",
    "```sh\n",
    "# como eu uso o VSCode\n",
    "code .\n",
    "```\n",
    "   - Editar items.py\n",
    "   \n",
    "   ```py\n",
    "   from scrapy import Item, Field\n",
    "   \n",
    "   class StackItem(Item):\n",
    "       title = Field()\n",
    "        url = Field()\n",
    "    ```\n",
    "    \n",
    "    - Criar o ficheiro \"stack_spider.py\" dentro da pasta spider\n",
    "    ```py\n",
    "    class StackSpider(Spider):\n",
    "    name = \"stack\"\n",
    "    allowed_domains = [\"stackoverflow.com\"]\n",
    "    start_urls = [\n",
    "        \"http://stackoverflow.com/questions?pagesize=50&sort=newest\",\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        questions = Selector(response).xpath('//div[@class=\"summary\"]/h3')\n",
    "\n",
    "        for question in questions:\n",
    "            item = StackItem()\n",
    "            item['title'] = question.xpath(\n",
    "                'a[@class=\"question-hyperlink\"]/text()').extract()[0]\n",
    "            item['url'] = question.xpath(\n",
    "                'a[@class=\"question-hyperlink\"]/@href').extract()[0]\n",
    "            yield item\n",
    "      ```\n",
    "    \n",
    "    - Testar se \"Spider\" esta a conectar com o site\n",
    "    ```sh\n",
    "    scrapy crawl stack\n",
    "    ```\n",
    "    \n",
    "    - Gravar as informações num arquivo JSON\n",
    "    ```sh\n",
    "    scrapy crawl stack -o itens.json -t json\n",
    "    ```\n",
    "    \n",
    "    - Configurar o scrapy de forma a guardar as informações diretamente no MongoDB\n",
    "        - Editar settings.py - adicionar\n",
    "        ```py\n",
    "        ITEM_PIPELINES = {'stack.pipelines.MongoDBPipeline':300}\n",
    "\n",
    "        MONGODB_SERVER = \"localhost\"\n",
    "        MONGODB_PORT = 27017\n",
    "        MONGODB_DB = \"stackoverflow\"\n",
    "        MONGODB_COLLECTION = \"questions\"\n",
    "        ```\n",
    "        - Editar o pipelines.py - adicionar\n",
    "        ```py\n",
    "        import pymongo\n",
    "        from scrapy.conf import settings\n",
    "        from scrapy.exceptions import DropItem\n",
    "        from scrapy import log\n",
    "        ```\n",
    "        ```py\n",
    "        class MongoDBPipeline(object):\n",
    "\n",
    "            def __init__(self):\n",
    "                connection = pymongo.MongoClient(\n",
    "                    settings['MONGODB_SERVER'],\n",
    "                    settings['MONGODB_PORT']\n",
    "                )\n",
    "                db = connection[settings['MONGODB_DB']]\n",
    "                self.collection = db[settings['MONGODB_COLLECTION']]\n",
    "\n",
    "            def process_item(self, item, spider):\n",
    "                valid = True\n",
    "                for data in item:\n",
    "                    if not data:\n",
    "                        valid = False\n",
    "                        raise DropItem(\"Missing {0}!\".format(data))\n",
    "                if valid:\n",
    "                    self.collection.insert(dict(item))\n",
    "                    log.msg(\"Question added to MongoDB database!\",\n",
    "                            level=log.DEBUG, spider=spider)\n",
    "                return item\n",
    "                ```\n",
    "         - Testar o Scrapy\n",
    "         ```sh\n",
    "         scrapy crawl stack\n",
    "         ```\n",
    "         \n",
    "         - Visualizar o dados no MongoDB atarvés do terminal ou usar Robomongo que agora de chama Studio 3T \n",
    "         \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
